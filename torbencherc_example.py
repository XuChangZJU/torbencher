from src.torbencherc import torbencherc

config = {
    "out_dir": "./results",
    "seed": 1234567890,
    "devices": [
        "mps",
        # other device names...
    ],
    "test_modules": [
         "torch.nn.functional",
        "torch.optim",
        "torch.special",
        "torch.utils.data",
        "torch.utils",
        "torch.autograd",
        "torch",
        "torch.nn",
        "torch.utils.mobile_optimizer",
        "torch.utils.checkpoint",
        "torch.export",
        "torch.profiler",
        "torch.backends",
        "torch.cpu",
        "torch.testing",
        "torch.nn.init",
        "torch.fft",
        "torch.autograd",

        "torch.nn.parallel",
        "torch.nn.utils",
        "torch.nn.modules",
        "torch.nn.functional",
        "torch.nn.utils.parametrize",
        "torch.nn.utils.prune",
        "torch.nn.utils.parametrizations",
        "torch.nn.utils.stateless",
        "torch.nn.utils.rnn",
        "torch.nn.modules.module",
        "torch.nn.modules.lazy",
        "torch.autograd.forward_ad",
        "torch.autograd.gradcheck",
        "torch.autograd.graph",
        "torch.autograd.torch",
        "torch.autograd.Function",
        "torch.autograd.profiler_util",
        "torch.autograd.grad_mode",
        "torch.autograd.profiler",
        "torch.autograd.functional",
        "torch.autograd.graph.Node",
        "torch.autograd.torch.Tensor",
        "torch.autograd.Function.FunctionCtx",
        "torch.autograd.profiler.profile",
        "torch.backends.opt_einsum",
        "torch.backends.mha",
        "torch.backends.nnpack",
        "torch.backends.mps",
        "torch.backends.cpu",
        "torch.backends.cuda",
        "torch.backends.mkl",
        "torch.backends.mkldnn",
        "torch.backends.openmp",
        "torch.backends.cudnn",
        "torch.backends.cuda.cufftplancache",
        "torch.backends.cuda.cufft_plan_cache",
        "torch.backends.cuda.cufftplancache.torch",
        "torch.backends.cuda.cufftplancache.torch.backends",
        "torch.backends.cuda.cufftplancache.torch.backends.cuda",
        "torch.backends.cuda.cufft_plan_cache.torch",
        "torch.backends.cuda.cufft_plan_cache.torch.backends",
        "torch.backends.cuda.cufft_plan_cache.torch.backends.cuda",
        "torch.optim.Optimizer",
        "torch.optim.lrscheduler",
        "torch.utils.data",
        "torch.utils.data._utils",
        "torch.utils.data.distributed",
        "torch.utils.data.torch",
        "torch.utils.data.utils",
        "torch.utils.data._utils.collate",
        "torch.utils.data.utils.collate",
        "torch.profiler.itt",
        "torch.export.dynamic_shapes",
        "torch.export.graph_signature"
    ],
    "format": "csv",
    "num_epoch": 1,
    "name_spec": "timestamp"
}

bencher = torbencherc(config)
result = bencher.run()
print(result)
